# ğŸ¤– RAGFlow + Ollama í†µí•© ê°€ì´ë“œ

ì´ ë¬¸ì„œëŠ” RAGFlowì— Ollamaë¥¼ í†µí•©í•˜ì—¬ ë¡œì»¬ LLM ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸš€ ê°œìš”

RAGFlow Docker Compose ì„¤ì •ì— **Ollama**ê°€ í†µí•©ë˜ì–´ ìˆì–´, **qwen3:14b** ëª¨ë¸ì„ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ê³  GPUì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“‹ ì‚¬ì „ ìš”êµ¬ì‚¬í•­

### í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­
- **GPU**: NVIDIA GPU (CUDA ì§€ì›)
- **VRAM**: ìµœì†Œ 16GB ê¶Œì¥ (qwen3:14b ëª¨ë¸ìš©)
- **RAM**: ìµœì†Œ 32GB ê¶Œì¥
- **ë””ìŠ¤í¬**: ìµœì†Œ 20GB ì—¬ìœ  ê³µê°„ (ëª¨ë¸ ë‹¤ìš´ë¡œë“œìš©)

### ì†Œí”„íŠ¸ì›¨ì–´ ìš”êµ¬ì‚¬í•­
- Docker Engine >= 24.0.0
- Docker Compose >= v2.26.1
- NVIDIA Container Toolkit (GPU ì§€ì›ìš©)

## ğŸ› ï¸ ì„¤ì¹˜ ë° ì„¤ì •

### 1. NVIDIA Container Toolkit ì„¤ì¹˜

#### Ubuntu/Debian
```bash
# NVIDIA Container Toolkit ì„¤ì¹˜
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
  sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
  sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit

# Docker ì¬ì‹œì‘
sudo systemctl restart docker
```

### 2. RAGFlow + Ollama ì‹¤í–‰

#### CPU + GPU í™˜ê²½ (ê¶Œì¥)
```bash
# GPU ë²„ì „ìœ¼ë¡œ ì‹¤í–‰
docker compose -f docker-compose-gpu.yml up -d
```

#### CPU ì „ìš© í™˜ê²½
```bash
# ì¼ë°˜ ë²„ì „ìœ¼ë¡œ ì‹¤í–‰ (GPU ì„¤ì •ì€ ë¬´ì‹œë¨)
docker compose -f docker-compose.yml up -d
```

## ğŸ“Š ì„œë¹„ìŠ¤ ëª¨ë‹ˆí„°ë§

### 1. Ollama ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
```bash
# ì»¨í…Œì´ë„ˆ ë¡œê·¸ í™•ì¸
docker logs ragflow-ollama

# ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
docker compose ps
```

### 2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì§„í–‰ìƒí™© í™•ì¸
```bash
# ì‹¤ì‹œê°„ ë¡œê·¸ ëª¨ë‹ˆí„°ë§
docker logs -f ragflow-ollama
```

ì˜ˆìƒ ë¡œê·¸ ì¶œë ¥:
```
ğŸš€ Starting Ollama server...
â³ Waiting for Ollama server to be ready...
âœ… Ollama server is ready!
ğŸ” Checking if qwen3:14b model exists...
ğŸ“¥ Downloading qwen3:14b model... (This may take a while)
âœ… qwen3:14b model downloaded successfully!
ğŸ§ª Testing qwen3:14b model...
âœ… qwen3:14b model is working correctly!
ğŸ‰ Ollama initialization completed!
ğŸ“Š Available models:
```

## ğŸ”§ ì„¤ì • ì»¤ìŠ¤í„°ë§ˆì´ì§•

### í™˜ê²½ë³€ìˆ˜ ì„¤ì •

`.env` íŒŒì¼ ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ ë‹¤ìŒ ê°’ë“¤ì„ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
# Ollama í¬íŠ¸ (ê¸°ë³¸ê°’: 11434)
OLLAMA_PORT=11434

# íƒ€ì„ì¡´ ì„¤ì •
TIMEZONE=Asia/Seoul
```

### ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©í•˜ê¸°

`docker/ollama-init.sh` íŒŒì¼ì„ ìˆ˜ì •í•˜ì—¬ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
# qwen3:14b ëŒ€ì‹  ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©
ollama pull llama3.1:8b
# ë˜ëŠ”
ollama pull codellama:7b
```

## ğŸŒ RAGFlowì—ì„œ Ollama ì‚¬ìš©í•˜ê¸°

### 1. Ollama ì—°ê²° ì„¤ì •

RAGFlow ì›¹ ì¸í„°í˜ì´ìŠ¤ì—ì„œ:
1. **Settings** â†’ **Model Configuration** ì´ë™
2. **Add LLM** í´ë¦­
3. ë‹¤ìŒ ì •ë³´ ì…ë ¥:
   - **Factory**: `Ollama`
   - **Base URL**: `http://ollama:11434`
   - **Model Name**: `qwen3:14b`

### 2. API ì§ì ‘ ì ‘ê·¼

```bash
# ëª¨ë¸ ëª©ë¡ í™•ì¸
curl http://localhost:11434/api/tags

# í…ŒìŠ¤íŠ¸ ìš”ì²­
curl http://localhost:11434/api/generate -d '{
  "model": "qwen3:14b",
  "prompt": "Hello, how are you?"
}'
```

## ğŸš¨ ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

#### 1. GPU ì¸ì‹ ì•ˆë¨
```bash
# NVIDIA ë“œë¼ì´ë²„ í™•ì¸
nvidia-smi

# Dockerì—ì„œ GPU ì ‘ê·¼ í™•ì¸
docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi
```

#### 2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨
```bash
# ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ
docker exec -it ragflow-ollama ollama pull qwen3:14b
```

#### 3. ë©”ëª¨ë¦¬ ë¶€ì¡±
- Docker Desktopì˜ ë©”ëª¨ë¦¬ ì œí•œ ì¦ê°€
- ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ í™•ì¸ ë° í™•ë³´

#### 4. ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë¬¸ì œ
```bash
# Ollama ì„œë¹„ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸
docker exec ragflow-server curl http://ollama:11434/api/tags
```

### ë¡œê·¸ ë¶„ì„

```bash
# ì „ì²´ ì„œë¹„ìŠ¤ ë¡œê·¸
docker compose logs

# Ollama ì „ìš© ë¡œê·¸
docker compose logs ollama

# RAGFlow ì„œë²„ ë¡œê·¸
docker compose logs ragflow
```

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

### 1. GPU ë©”ëª¨ë¦¬ ìµœì í™”
```bash
# GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi
```

### 2. ëª¨ë¸ ìºì‹œ ê´€ë¦¬
```bash
# ëª¨ë¸ ëª©ë¡ í™•ì¸
docker exec ragflow-ollama ollama list

# ë¶ˆí•„ìš”í•œ ëª¨ë¸ ì‚­ì œ
docker exec ragflow-ollama ollama rm <model_name>
```

## ğŸ”„ ì—…ë°ì´íŠ¸ ë° ìœ ì§€ë³´ìˆ˜

### ëª¨ë¸ ì—…ë°ì´íŠ¸
```bash
# ìµœì‹  ëª¨ë¸ë¡œ ì—…ë°ì´íŠ¸
docker exec ragflow-ollama ollama pull qwen3:14b

# ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘
docker compose restart ollama
```

### ë°ì´í„° ë°±ì—…
```bash
# Ollama ë°ì´í„° ë°±ì—…
docker run --rm -v ragflow_ollama_data:/data -v $(pwd):/backup ubuntu tar czf /backup/ollama_backup.tar.gz /data
```

## ğŸ“š ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- [Ollama ê³µì‹ ë¬¸ì„œ](https://ollama.ai/)
- [qwen3 ëª¨ë¸ ì •ë³´](https://ollama.ai/library/qwen2.5)
- [RAGFlow ë¬¸ì„œ](https://ragflow.io/docs/)
- [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)